<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content=" 本文选自《从零实现分布式 KV》课程的加餐文章。 从零开始，手写基于 raft 的分布式 KV 系统，课程详情可以看这里：https://av6huf2e1k.feishu.cn/docx/JCssdlgF4oRADcxxLqncPpRCn5b\n在简历上如何写这个项目？ 项目概述\n基于 MIT 6824 课程 lab 框架，实现一个基于 raft 共识算法、高性能、可容错的分布式 KV 存储系统，保证系统的一致性和可靠性。\n"><title>分布式 KV 面试汇总</title>
<link rel=canonical href=https://roseduan.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F-kv-%E9%9D%A2%E8%AF%95%E6%B1%87%E6%80%BB/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="分布式 KV 面试汇总"><meta property='og:description' content=" 本文选自《从零实现分布式 KV》课程的加餐文章。 从零开始，手写基于 raft 的分布式 KV 系统，课程详情可以看这里：https://av6huf2e1k.feishu.cn/docx/JCssdlgF4oRADcxxLqncPpRCn5b\n在简历上如何写这个项目？ 项目概述\n基于 MIT 6824 课程 lab 框架，实现一个基于 raft 共识算法、高性能、可容错的分布式 KV 存储系统，保证系统的一致性和可靠性。\n"><meta property='og:url' content='https://roseduan.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F-kv-%E9%9D%A2%E8%AF%95%E6%B1%87%E6%80%BB/'><meta property='og:site_name' content='roseduan'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='KV 存储'><meta property='article:tag' content='分布式 KV'><meta property='article:published_time' content='2024-03-04T22:51:56+08:00'><meta property='article:modified_time' content='2024-03-04T22:51:56+08:00'><meta name=twitter:title content="分布式 KV 面试汇总"><meta name=twitter:description content=" 本文选自《从零实现分布式 KV》课程的加餐文章。 从零开始，手写基于 raft 的分布式 KV 系统，课程详情可以看这里：https://av6huf2e1k.feishu.cn/docx/JCssdlgF4oRADcxxLqncPpRCn5b\n在简历上如何写这个项目？ 项目概述\n基于 MIT 6824 课程 lab 框架，实现一个基于 raft 共识算法、高性能、可容错的分布式 KV 存储系统，保证系统的一致性和可靠性。\n"><link rel="shortcut icon" href=/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu7339003300074909980.png width=300 height=308 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🤟</span></figure><div class=site-meta><h1 class=site-name><a href=/>roseduan</a></h1><h2 class=site-description>stay hungry, stay foolish</h2></div></header><ol class=menu-social><li><a href=https://space.bilibili.com/26194591 target=_blank title="B 站「程序员roseduan」" rel=me><!doctype html><svg t="1712105268862" class="icon" viewBox="0 0 1024 1024" p-id="5725" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M1019.54782609 345.3106087c-3.20556522-142.1133913-127.15408696-169.36069565-127.15408696-169.36069566s-96.70121739-.53426087-222.25252174-1.60278261l91.3586087-88.15304347s14.42504348-18.16486957-10.15095652-38.46678261c-24.576-20.30191304-26.17878261-11.21947826-34.72695653-5.87686957-7.47965217 5.3426087-117.00313043 112.72904348-136.23652174 131.96243479-49.68626087.0-101.50956522-.53426087-151.73008695-.53426087h17.63060869S315.392 43.98747826 306.84382609 38.1106087s-9.61669565-14.42504348-34.72695652 5.87686956c-24.576 20.30191304-10.15095652 38.46678261-10.15095653 38.46678261l93.49565218 90.82434783c-101.50956522.0-189.12834783.53426087-229.73217392 2.13704347C-5.69878261 213.34817391 4.45217391 345.3106087 4.45217391 345.3106087s1.60278261 283.15826087.0 426.34017391c14.42504348 143.18191304 124.48278261 166.15513043 124.48278261 166.15513043s43.8093913 1.06852174 76.39930435 1.06852174c3.20556522 9.08243478 5.87686957 53.96034783 56.0973913 53.96034783 49.68626087.0 56.0973913-53.96034783 56.09739131-53.96034783s365.96869565-1.60278261 396.42156522-1.60278261c1.60278261 15.49356522 9.08243478 56.63165217 59.30295652 56.09739131 49.68626087-1.06852174 53.42608696-59.30295652 53.42608695-59.30295652s17.09634783-1.60278261 67.85113044.0c118.60591304-21.90469565 125.55130435-160.81252174 125.55130435-160.81252174s-2.13704348-285.82956522-.53426087-427.94295652zM917.504 798.36382609c0 22.43895652-17.6306087 40.60382609-39.53530435 40.60382608H156.71652174c-21.90469565.0-39.53530435-18.16486957-39.53530435-40.60382608V320.20034783c0-22.43895652 17.6306087-40.60382609 39.53530435-40.60382609h721.25217391c21.90469565.0 39.53530435 18.16486957 39.53530435 40.60382609v478.16347826z" fill="#8a8a8a" p-id="5726"/><path d="M409.088 418.816l-203.264 38.912 17.408 76.288 201.216-38.912zm109.568 202.24c-49.664 106.496-94.208 26.112-94.208 26.112l-33.28 21.504s65.536 89.6 128 21.504c73.728 68.096 130.048-22.016 130.048-22.016l-30.208-19.456c0-.512-52.736 75.776-100.352-27.648zM619.008 495.104l201.728 38.912 16.896-76.288-202.752-38.912z" fill="#8a8a8a" p-id="5727"/></svg></a></li><li><a href=https://github.com/roseduan target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/roseduan520 target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://www.zhihu.com/people/roseduan target=_blank title=Zhihu rel=me><!doctype html><svg t="1704259577746" class="icon" viewBox="0 0 1024 1024" p-id="5040" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M570.581333 806.272h61.952L652.928 876.117333 764.074667 806.272h130.986666V230.186667h-324.48V806.272zM636.501333 292.693333h192.64V743.68h-73.898666l-73.813334 46.378667L668.032 743.808l-31.530667-.128V292.736zM515.754667 493.738667H377.429333a2999.466667 2999.466667.0 005.802667-194.56h135.338667S523.776 239.445334 495.872 240.128H261.76c9.216-34.730667 20.821333-70.613333 34.688-107.690667.0.0-63.701333.0-85.333333 57.130667C202.112 213.12 176.128 303.786667 129.877333 396.416c15.573333-1.706667 67.114667-3.114667 97.450667-58.794667 5.589333-15.616 6.656-17.621333 13.568-38.485333h76.373333c0 27.776-3.157333 177.109333-4.437333 194.474667h-138.24c-31.104.0-41.173333 62.549333-41.173333 62.549333h173.482666C295.253333 688.256 232.789333 799.573333 119.466667 887.466667c54.186667 15.488 108.202667-2.432 134.912-26.197334.0.0 60.8-55.338667 94.122666-183.381333L491.264 849.834667s20.906667-71.168-3.285333-105.856c-20.053333-23.637333-74.24-87.552-97.322667-110.72l-38.698667 30.72c11.52-36.992 18.474667-72.96 20.821334-107.690667h163.072s-.213333-62.549333-20.053334-62.549333z" p-id="5041" fill="#8a8a8a"/></svg></a></li><li><a href=https://mp.weixin.qq.com/s/hs36eiU7Zr-UdJ0YBtZgKQ target=_blank title="微信公众号 roseduan写字的地方" rel=me><!doctype html><svg t="1704259948565" class="icon" viewBox="0 0 1194 1024" p-id="5272" xmlns:xlink="http://www.w3.org/1999/xlink" width="233.203125" height="200"><path d="M728.064 535.296a35.498667 35.498667.0 11-70.912.0 35.498667 35.498667.0 0170.912.0m246.016.0a35.498667 35.498667.0 11-70.997333.0 35.498667 35.498667.0 0170.997333.0" fill="#8a8a8a" p-id="5273"/><path d="M902.144 930.133333l-6.656 1.450667a594.176 594.176.0 01-18.602667 3.669333c-26.453333 4.778667-44.629333 6.826667-64 6.144C645.034666 935.850666 514.218666 853.504 461.824 722.688a446.464 446.464.0 01-6.826667-19.114667l-1.962666-6.058666a199.68 199.68.0 01-3.84-13.653334 338.858667 338.858667.0 01-9.216-75.690666c0-171.008 157.013333-305.92 354.304-314.709334l8.874666-.682666c7.850667-.597333 12.970667-.853333 18.773334-.853334h14.762666l9.301334.170667c188.16 14.677333 340.394667 156.672 340.394666 323.925333.0 78.336-38.826667 155.733333-109.653333 222.293334a303.530667 303.530667.0 01-7.082667 6.4l9.984 71.082666a49.152 49.152.0 01-69.12 58.453334l-98.816-46.421334a582.485333 582.485333.0 01-9.472 2.304zm220.16-314.026666c0-131.754667-124.586667-247.978667-279.125333-260.096H821.930667c-3.754667.0-7.68.085333-13.994667.597333l-10.666667.768C631.466667 364.8 503.978667 474.453333 503.978667 608.256c0 18.773333 2.730667 40.874667 7.509333 60.842667.597333 2.474667 1.450667 5.632 2.56 9.216l1.706667 4.949333c2.133333 6.570667 4.266667 12.629333 5.632 15.957333 42.325333 105.728 149.930667 173.397333 293.717333 178.176 13.738667.512 28.16-1.109333 50.346667-5.12a531.626667 531.626667.0 0016.64-3.242666l5.632-1.28c4.693333-1.109333 8.448-1.962667 19.370666-4.778667a32 32 0 0121.418667 2.133333L1013.76 905.216l-9.984-69.290667a32 32 0 0113.909333-31.232c1.194667-.853333 8.618667-6.912 15.189334-13.056 58.709333-55.210667 89.429333-116.394667 89.429333-175.616zM12.970667 378.197333C12.970667 171.52 206.677333 5.376 442.709333 5.376c208.384.0 394.24 130.304 431.872 306.090667 1.28 5.973333 1.28 11.264.512 16.896a32 32 0 01-63.658666-5.973334C779.178667 179.2 621.482667 69.376 442.709333 69.376c-202.666666.0-365.738666 139.776-365.738666 308.906667.0 89.941333 46.165333 171.52 136.021333 237.568a32 32 0 0111.434667 35.84l-23.296 69.973333 104.96-48.896a32 32 0 0122.442666-1.706667l5.461334 1.706667c54.357333 11.093333 76.288 14.506667 108.714666 14.506667a200.789333 200.789333.0 0032.256-4.010667c-.512.170667-1.536.512-2.730666 1.536a33.109333 33.109333.0 0139.253333 1.109333A32 32 0 01516.864 730.88c-11.776 14.848-47.36 20.309333-74.24 20.309333-37.12.0-62.037333-3.584-119.893333-15.530666L198.570667 793.6a49.493333 49.493333.0 01-68.949334-59.733333l26.88-80.64c-93.525333-75.52-143.530666-170.24-143.530666-275.029334z" fill="#8a8a8a" p-id="5274"/><path d="M344.576 254.378667a44.373333 44.373333.0 11-88.746667.085333 44.373333 44.373333.0 0188.746667.0M636.586667 254.378667A44.373333 44.373333.0 11547.84 254.464a44.373333 44.373333.0 0188.746667.0" fill="#8a8a8a" p-id="5275"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 7a4 4 0 108 0A4 4 0 008 7"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#在简历上如何写这个项目>在简历上如何写这个项目？</a></li><li><a href=#可能的面试问题回答>可能的面试问题&回答</a><ol><li><a href=#raft-主要在什么场景中使用>Raft 主要在什么场景中使用？</a></li><li><a href=#raft-为了简洁性做了哪些牺牲即有哪些性能问题>Raft 为了简洁性做了哪些牺牲（即有哪些性能问题）？</a></li><li><a href=#raft-在选举时是不能正常对外提供服务的这在工程中影响大吗>Raft 在选举时是不能正常对外提供服务的，这在工程中影响大吗？</a></li><li><a href=#有其他不基于-leader-的共识协议吗>有其他不基于 Leader 的共识协议吗？</a></li><li><a href=#论文提到-raft-只在非拜占庭的条件下才能正常工作什么是拜占庭条件为什么-raft-会出问题>论文提到 Raft 只在非拜占庭的条件下才能正常工作，什么是拜占庭条件？为什么 Raft 会出问题？</a></li><li><a href=#通常来说raft-的所有节点都期望部署在一个数据中心吗>通常来说，Raft 的所有节点都期望部署在一个数据中心吗？</a></li><li><a href=#如果发生网络分区raft-会出现两个-leader-即脑裂的情况吗>如果发生网络分区，Raft 会出现两个 Leader ，即脑裂的情况吗？</a></li><li><a href=#当集群中有些-peer-宕机后此时的多数派是指所有节点的多数还是指存活节点的多数>当集群中有些 Peer 宕机后，此时的“多数派”是指所有节点的多数，还是指存活节点的多数？</a></li><li><a href=#选举超时间隔选择的过短会有什么后果会导致-raft-算法出错吗>选举超时间隔选择的过短会有什么后果？会导致 Raft 算法出错吗？</a></li><li><a href=#为什么使用随机超时间隔>为什么使用随机超时间隔？</a></li><li><a href=#candidate-可以在收到多数票后不等其余-follower-的回复就直接变成-leader-吗>Candidate 可以在收到多数票后，不等其余 Follower 的回复就直接变成 Leader 吗？</a></li><li><a href=#raft-对网络有什么假设>Raft 对网络有什么假设？</a></li><li><a href=#votedfor-在-requestvote-rpc-中起什么作用>votedFor 在 requestVote RPC 中起什么作用？</a></li><li><a href=#即使服务器不宕机leader-也可能会下台吗>即使服务器不宕机，Leader 也可能会下台吗？</a></li><li><a href=#如果-raft-进群中有过半数的-peer-宕机会发生什么>如果 Raft 进群中有过半数的 Peer 宕机会发生什么？</a></li><li><a href=#请简单说说-raft-中的选举流程>请简单说说 Raft 中的选举流程？</a></li><li><a href=#如果所有-peer-初始化时不为-follower而都是-candidate其他部分保持不变算法还正确吗>如果所有 Peer 初始化时不为 Follower、而都是 Candidate，其他部分保持不变，算法还正确吗？</a></li><li><a href=#如何避免出现网络分区的-peer-恢复通信时将整体-term-推高>如何避免出现网络分区的 Peer 恢复通信时将整体 Term 推高？</a></li><li><a href=#raft-和-paxos-有什么区别>Raft 和 Paxos 有什么区别？</a></li><li><a href=#raft-在工程中有哪些常见的优化>Raft 在工程中有哪些常见的优化？</a></li><li><a href=#请简单描述基于-raft-的分布式-kv-系统的架构>请简单描述基于 raft 的分布式 KV 系统的架构？</a></li><li><a href=#分布式系统中读数据的流程是什么样的如何优化>分布式系统中读数据的流程是什么样的，如何优化？</a></li><li><a href=#客户端发送请求的时候如何知道集群中的-leader-节点是哪个>客户端发送请求的时候，如何知道集群中的 Leader 节点是哪个？</a></li><li><a href=#如果-raft-集群的-leader-节点发生故障客户端如何处理>如果 raft 集群的 Leader 节点发生故障，客户端如何处理？</a></li><li><a href=#如何处理客户端的重复请求>如何处理客户端的重复请求？</a></li><li><a href=#shardkv-的问题为什么需要对分布式-kv-系统进行分片>Shardkv 的问题：为什么需要对分布式 KV 系统进行分片？</a></li><li><a href=#shardkv-的配置怎么保存>Shardkv 的配置怎么保存？</a></li><li><a href=#shard-数据如何迁移>Shard 数据如何迁移？</a></li><li><a href=#shard-迁移的时候客户端的请求会受到影响吗>Shard 迁移的时候，客户端的请求会受到影响吗？</a></li><li><a href=#如果有并发的客户端请求和-shard-迁移请求应该怎么处理>如果有并发的客户端请求和 shard 迁移请求，应该怎么处理？</a></li><li><a href=#如果某个-shard-已经迁移了那么它还会占存储空间吗>如果某个 Shard 已经迁移了，那么它还会占存储空间吗？</a></li></ol></li><li><a href=#参考资料>参考资料</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/kv-%E5%AD%98%E5%82%A8/>KV 存储
</a><a href=/categories/%E5%88%86%E5%B8%83%E5%BC%8F-kv/>分布式 KV</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E5%88%86%E5%B8%83%E5%BC%8F-kv-%E9%9D%A2%E8%AF%95%E6%B1%87%E6%80%BB/>分布式 KV 面试汇总</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 04, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 13 分钟</time></div></footer></div></header><section class=article-content><blockquote><p>本文选自《从零实现分布式 KV》课程的加餐文章。 从零开始，手写基于 raft 的分布式 KV 系统，课程详情可以看这里：<a class=link href="https://link.zhihu.com/?target=https%3A//av6huf2e1k.feishu.cn/docx/JCssdlgF4oRADcxxLqncPpRCn5b" target=_blank rel=noopener>https://av6huf2e1k.feishu.cn/docx/JCssdlgF4oRADcxxLqncPpRCn5b</a></p></blockquote><ol><li><h2 id=在简历上如何写这个项目>在简历上如何写这个项目？</h2></li></ol><p><strong>项目概述</strong></p><p>基于 MIT 6824 课程 lab 框架，实现一个基于 raft 共识算法、高性能、可容错的分布式 KV 存储系统，保证系统的一致性和可靠性。</p><p><strong>设计细节</strong></p><ul><li>设计基于 Raft 一致性算法的分布式系统架构。</li><li>支持分布式数据存储和检索的 KV 存储引擎，采用 Raft 协议确保数据的强一致性。</li><li>实现数据分片和自动故障转移机制，以实现系统的高可用性和容错性。</li><li>使用 Go 语言编写，工程级代码可靠性和简洁性。</li></ul><p><strong>结果</strong></p><ul><li>参照 Raft 论文使用 Golang 实现了领导选举、日志同步、宕机重启和日志压缩等主要功能。熟悉 Raft 算法的基本原理和实现细节，熟悉 Golang 并发编程和分布式调试。</li><li>实现了一个高性能的分布式键值存储系统，保证数据的一致性和可靠性。</li><li>通过所有代码测试，在负载测试中表现出良好的性能和稳定性，能够有效地应对并发访问和故障情况。</li></ul><ol><li><h2 id=可能的面试问题回答>可能的面试问题&回答</h2></li></ol><blockquote><p>以下我们每个节点统称为 Peer，面试官可能会叫节点、副本(Replica)、Node 等等术语，记得和面试官对齐就好。</p></blockquote><h3 id=raft-主要在什么场景中使用>Raft 主要在什么场景中使用？</h3><p>通常有两种用途：</p><ol><li><strong>元信息服务</strong>，也称为配置服务（configuration services）、分布式协调服务（coordinator services）等等。如 etcd。用以追踪集群中元信息（比如副本位置等等）、多副本选主、通知（Watch）等等。</li><li><strong>数据<strong><strong>复制</strong></strong>（Data replication）</strong>。如 TiKV、CockroachDB 和本课程中的 ShardKV，使用 Raft 作为数据复制和冗余的一种手段。与之相对，GFS 使用简单的主从复制的方法来冗余数据，可用性和一致性都比 Raft 要差。</li></ol><blockquote><p>注：在分布式系统中，数据指的是外界用户存在系统中的数据；元数据指的是用户维护集群运转的内部信息，比如有哪些机器、哪些副本放在哪里等等。</p></blockquote><h3 id=raft-为了简洁性做了哪些牺牲即有哪些性能问题>Raft 为了简洁性做了哪些牺牲（即有哪些性能问题）？</h3><ol><li><strong>每个操作都要落盘</strong>。如果想提高性能可能需要将多个操作 batch 起来。</li><li><strong>主从同步数据较慢</strong>。在每个 Leader 和 Follower 之间只允许有一个已经发出 AppendEntries；只有收到确认了，Leader 才能发下一个。类似 TCP 中的“停等协议”。如果写入速度较大，可能将所有的 AppendEntries Pipeline 起来性能会好一些（即 Leader 不等收到上一个 AppendEntries 的 RPC Reply，就开始发下一个）</li><li><strong>只支持全量快照</strong>。如果状态机比较小这种方式还可以接受，如果数据量较大，就得支持增量快照。</li><li><strong>全量快照同步代价大</strong>。如果快照数据量很大，每次全量发送代价会过高。尤其是如果 Follower 本地有一些较老的快照时，我们只需要发增量部分即可。</li><li><strong>难以利用多核</strong>。因为 log 只有一个写入点，所有操作都得抢这一个写入点。</li></ol><h3 id=raft-在选举时是不能正常对外提供服务的这在工程中影响大吗>Raft 在选举时是不能正常对外提供服务的，这在工程中影响大吗？</h3><p>不太大，因为只有网络故障、机器宕机等事件才会引起宕机。这些故障的发生率可能在数天到数月一次，但 Raft 选主在秒级就能完成。因此，在实践中，这通常不是一个问题。</p><h3 id=有其他不基于-leader-的共识协议吗>有其他不基于 Leader 的共识协议吗？</h3><p>原始的 Paxos 就是无主的（区别于有主的 MultiPaxos）。因此不会有选举时的服务停顿，但也有代价——每次数据同步时都要达成共识，则数据同步代价会更大（所需要的 RPC 更多，因为每次同步消息都是两阶段的）。</p><h3 id=论文提到-raft-只在非拜占庭的条件下才能正常工作什么是拜占庭条件为什么-raft-会出问题>论文提到 Raft 只在非拜占庭的条件下才能正常工作，什么是拜占庭条件？为什么 Raft 会出问题？</h3><p>“非拜占庭条件”（Non-Byzantine conditions）是指所有的服务器都是“宕机-停止”（ fail stop）模型（更多模型参见<a class=link href="https://ddia.qtmuniao.com/#/ch08?id=%e7%b3%bb%e7%bb%9f%e6%a8%a1%e5%9e%8b%e5%92%8c%e7%8e%b0%e5%ae%9e" target=_blank rel=noopener>这里</a>）：即每个服务器要么严格遵循 Raft 协议，要么停止服务。例如，服务器断电就是一个非拜占庭条件，此时服务器会停止执行指令，则 Raft 也会停止运行，且不会发送错误结果给客户端。</p><p>拜占庭故障（Byzantine failure）是指有些服务器不好好干活了——可能是代码因为有 bug，也可能是混入了恶意节点。如果出现这种类型节点，Raft 可能会发送错误的结果给客户端。</p><h3 id=通常来说raft-的所有节点都期望部署在一个数据中心吗>通常来说，Raft 的所有节点都期望部署在一个数据中心吗？</h3><p>是的。跨数据中心的部署可能会有一些问题。有些系统，如原始的 Paxos（由于是 Leaderless）可以跨数据中心部署。因为客户端可以和本地的 Peer 进行通信。</p><h3 id=如果发生网络分区raft-会出现两个-leader-即脑裂的情况吗>如果发生网络分区，Raft 会出现两个 Leader ，即脑裂的情况吗？</h3><p>不会，被分到少数派分区的 Leader 会发现日志不能同步到大多数节点，从而不能提交任何日志。一种优化是，如果一个 Leader 持续不能联系到多数节点，就自动变为 Follower。</p><h3 id=当集群中有些-peer-宕机后此时的多数派是指所有节点的多数还是指存活节点的多数>当集群中有些 Peer 宕机后，此时的“多数派”是指所有节点的多数，还是指存活节点的多数？</h3><p>所有节点的多数。比如集群总共有五个 Peer，则多数派永远是指不低于 3 个 Peer。</p><p>如果是后者，考虑这样一个例子。集群中有五个 Peer，有两个 Peer 被分到一个分区，他们就会认为其他三个 Peer 都宕机了，则这两个 Peer 仍然会选出 Leader ，这明显是不符合预期的。</p><h3 id=选举超时间隔选择的过短会有什么后果会导致-raft-算法出错吗>选举超时间隔选择的过短会有什么后果？会导致 Raft 算法出错吗？</h3><p>选举超时间隔选的不好，只会影响服务的可用性（liveness），而不会影响正确性（safety）。</p><p>如果选举间隔过小，则所有的 Follower 可能会频繁的发起选举。这样，Raft 的时间都耗在了选举上，而不能正常的对外提供服务。</p><p>如果选举间隔过大，则当老 Leader 故障之后、新 Leader 当选之前，会有一个不必要的过长等待。</p><h3 id=为什么使用随机超时间隔>为什么使用随机超时间隔？</h3><p>为了避免多个 Candidate 一直出现平票的情况，导致一直选不出主。</p><h3 id=candidate-可以在收到多数票后不等其余-follower-的回复就直接变成-leader-吗>Candidate 可以在收到多数票后，不等其余 Follower 的回复就直接变成 Leader 吗？</h3><p>可以的。首先，多数票就足够成为主了；其次，想等所有票也是不对的，因为可能有些 Peer 已经宕机或者发生网络隔离了。</p><h3 id=raft-对网络有什么假设>Raft 对网络有什么假设？</h3><p>网络是不可靠的：可能会丢失 RPC 请求和回复，也可能会经历任意延迟后请求才到达。</p><p>但网络是有界的（bounded）：在一段时间内请求总会到达，如果还不到达，我们就认为该 RPC 丢失了。</p><h3 id=votedfor-在-requestvote-rpc-中起什么作用>votedFor 在 requestVote RPC 中起什么作用？</h3><p>保证每个 Peer 在一个 Term 中只能投一次票。即，如果在某个 term 中，出现了两个 Candidate，那么 Follower 只能投其中一人。</p><p>且 votedFor 要进行持久化，即不能说某个 Peer 之前投过一次票，宕机重启后就又可以投票了。</p><h3 id=即使服务器不宕机leader-也可能会下台吗>即使服务器不宕机，Leader 也可能会下台吗？</h3><p>是的，比如说 Leader 所在服务器可能 CPU 负载太高、响应速度过慢，或者网络出现故障，或者丢包太严重，都有可能造成其他 Peer 不能及时收到其 AppendEntries，从而造成超时，发起选举。</p><h3 id=如果-raft-进群中有过半数的-peer-宕机会发生什么>如果 Raft 进群中有过半数的 Peer 宕机会发生什么？</h3><p>Raft 集群不能正常对外提供服务。所有剩余的节点会不断尝试发起选举，但都由于不能获得多数票而当选。</p><p>但只要有足够多的服务器恢复正常，就能再次选出 Leader，继续对外提供服务。</p><h3 id=请简单说说-raft-中的选举流程>请简单说说 Raft 中的选举流程？</h3><p>所有的 Peer 都会初始化为 Follower，且每个 Peer 都会有一个内置的选举超时的 Timer。</p><p>当一段时间没有收到领导者的心跳或者没有投给其他 Candidate 票时，选举时钟就会超时。</p><p>该 Peer 就会由 Follower 变为 Candidate，Term++，然后向其他 Peer 要票（带上自己的 Term 和最后一条日志信息）</p><p>其他 Peer 收到请求后，如果发现 Term 不大于该 Candidate、日志也没有该 Candidate 新、本 Term 中也没有投过票，就投给该 Term 票。</p><p>如果该 Peer 能收集到多数票，则为成为 Leader。</p><h3 id=如果所有-peer-初始化时不为-follower而都是-candidate其他部分保持不变算法还正确吗>如果所有 Peer 初始化时不为 Follower、而都是 Candidate，其他部分保持不变，算法还正确吗？</h3><p>正确，但是效率会变低一些。</p><p>因为这相当于在原来的基础上，所有 Peer 的第一轮选举超时是一样：同时变为 Candidate。则谁都要不到多数票，会浪费一些时间。之后就又会变成原来的选举流程。</p><h3 id=如何避免出现网络分区的-peer-恢复通信时将整体-term-推高>如何避免出现网络分区的 Peer 恢复通信时将整体 Term 推高？</h3><p><strong>问题解释</strong>：如果某个 Peer （我们不妨称其为 A）和其他 Peers 隔离后，也就是出现了网络分区，会不断推高 Term，发起选举。由于持续要不到其他 Peer 的票，因此会持续推高 Term。一旦其之后某个时刻恢复和其他 Peer 的通信，而由于 Term 是 Raft 中的<a class=link href=https://av6huf2e1k.feishu.cn/docx/ZYMGdQMA2ouPnKxfh56cUvpwnAg#BkIfd6wS8o2x1LxstRYcr82PnSe target=_blank rel=noopener>第一优先级</a>，因此会强迫当前的 Leader 下台。但问题是，由于在隔离期间日志被落下很多，Peer A 通常也无法成为 Leader。最终结果大概率是原来的 Leader 的 Term 被拉上来之后，重新当选为 Leader。有的人也将这个过程形象的称之为“惊群效应”。</p><p><strong>解决办法</strong>：<strong>PrevVote</strong>。每次 Candidate 发起选举时，不再推高 Term，但是会拿着 Term+1 去跟其他 Peer 要票，如果能要到合法的票数，再去推高 Term（Term+1）。而如果能要到多数票，其实就保证该 Candidate 没有发生网络隔离、日志是最新的。如果要不到多数票，就不能推高 Term，这样会保证发生了网络隔离的 Peer 不会一直推高自己的 Term。</p><h3 id=raft-和-paxos-有什么区别>Raft 和 Paxos 有什么区别？</h3><p>首先，Raft 和 Paxos 都是共识协议，而所有的共识协议在原理上都可以等价为 Paxos，所以才有共识协议本质上都是 Paxos 一说。</p><p>如 Raft 论文中提到的，Raft 是为了解决 Paxos 理解和实现都相对复杂的问题。将共识协议拆成两个相对独立的过程：领导者选举和日志复制，以降低理解和实现的复杂度。当然，如果要想工程可用，Raft 的优化也是无止境的大坑，也并非像论文声称的那么简单。因此，有人说，Raft 看起来简单只是因为论文叙述的更清楚，而非算法本身更为简洁。</p><p>Raft 其实是和 Multi-Paxos 等价，因为 Paxos 只解决单个值的共识问题。</p><p>Raft 和 Paxos 的角色分法也不太相同，Raft 的每个 Peer 都可以有 Leader，Candidate 和 Follower 三种状态；而 Paxos 是将系统分为 Proposer，Acceptor 和 Learner 三种角色，实现时可以按需组合角色。</p><p>在 Paxos 中，一旦某个日志在多数节点存在后就可以安全的提交；但在 Raft 中，不总是这样，比如一条日志在多数节点中存在后，但不是当前 Leader 任期的日志，也不能进行直接提交；而只能通过提交当前任期的日志来间接提交。</p><p>在Paxos 在选举时，Leader 可能需要借机补足日志，但 Raft 中选举过程完全不涉及日志复制（这也是 Raft 进行拆分的初衷）。这是因为 Raft 只允许具有最新日志的 Candidate 成为 Leader，而 Paxos 不限制这一点。</p><p>在 Paxos 中，允许乱序 commit 日志，而 Raft 只允许顺序提交。</p><p>在 Paxos 中，每个 Peer 的 term 是不一致的，全局自增的；在 Raft 中 term 是每个 Peer 独立自增的，但需要对齐。</p><p>更多区别，可以参考文末给出的资料。</p><h3 id=raft-在工程中有哪些常见的优化>Raft 在工程中有哪些常见的优化？</h3><p>由于领导者选举是个低频操作，主要 IO 路径优化还是集中在日志同步流程上。</p><ul><li><strong>batch</strong>：Leader 每次攒一批再刷盘和对 Follower 进行同步。降低刷盘和 RPC 开销。</li><li><strong>pipeline</strong>：每次发送日志时类似 TCP 的“停-等”协议，收到 Follower 确认后才更新 nextIndex，发送后面日志。其实可以改成流水线式的，不等前面日志确认就更新 nextIndex 继续发后面的。当然，如果后面发现之前日志同步出错，就要回退 nextIndex 重发之前日志——而原始版本 nextIndex 在<strong>同步阶段</strong>是单调递增的。</li><li><strong>并行 append</strong>：Leader 在 append 日志到本地前，就先发送日志给所有 Follower。</li></ul><h3 id=请简单描述基于-raft-的分布式-kv-系统的架构>请简单描述基于 raft 的分布式 KV 系统的架构？</h3><p>一个基于 raft 的分布式 KV 系统，实际上是由一组使用 raft 算法进行状态复制的节点组成。客户端会选择将请求发送到 Leader 节点，然后由 Leader 节点进行状态复制，即发送日志，当收到多数的节点成功提交日志的响应之后，Leader 会更新自己的 commitIndex，表示这条日志提交成功，并且 apply 到状态机中，然后返回结果给客户端。</p><p>以上是单个 raft 集群的分布式 KV 系统架构。</p><p>如果系统中数据量较大，一个 raft 集群可能无法承受大量的数据，性能也会受到影响。因此还基于此设计了可分片的分布式 shardkv 系统。shardkv 由多个 raft 集群组成，每个集群负责一部分 shard 数据。</p><p>Shard 到 raft 集群的映射关系，保存在独立的配置服务中。</p><h3 id=分布式系统中读数据的流程是什么样的如何优化>分布式系统中读数据的流程是什么样的，如何优化？</h3><p>为了保证线性一致性，目前的实现是利用了 raft 算法，将读请求传入到 raft 并进行状态复制，这样能够保证读到的数据一定是最新的。</p><p>但是由于读请求也进行了一次日志复制，执行效率会受到影响，业界常用的两种优化方式是 ReadIndex 和 LeaseRead。</p><blockquote><p><a class=link href=https://cn.pingcap.com/blog/linearizability-and-raft/ target=_blank rel=noopener>https://cn.pingcap.com/blog/linearizability-and-raft/</a></p><p><a class=link href=https://www.sofastack.tech/blog/sofa-jraft-linear-consistent-read-implementation/ target=_blank rel=noopener>https://www.sofastack.tech/blog/sofa-jraft-linear-consistent-read-implementation/</a></p></blockquote><h3 id=客户端发送请求的时候如何知道集群中的-leader-节点是哪个>客户端发送请求的时候，如何知道集群中的 Leader 节点是哪个？</h3><p>在没有任何前置条件的情况下，客户端会轮询集群中的每个节点并发送请求，如果非 Leader 节点收到请求，会返回一个错误给客户端。客户端然后挑选下一个 server 进行重试，直到得到了正确的响应。</p><p>然后会将 Leader 节点的 id 保存起来，下次发送请求的时候，优先选择这个节点发送。</p><h3 id=如果-raft-集群的-leader-节点发生故障客户端如何处理>如果 raft 集群的 Leader 节点发生故障，客户端如何处理？</h3><p>对于一个可容错的分布式 KV 系统，需要能够应对这种故障发生，并且在多数节点正常的情况下，需要依然提供服务。</p><p>得益于 raft 共识算法的特性，在某个节点故障后，其他节点会由于收不到心跳消息而超时，并重新发起选举。</p><p>所以客户端会在得不到正常响应的时候轮询重试，直到 raft 集群中的 Leader 节点重新选举完成并提供正常服务。</p><h3 id=如何处理客户端的重复请求>如何处理客户端的重复请求？</h3><p>如果客户端的请求已经提交，但是 server 返回的过程中结果丢失，那么客户端会发起重试，导致这个请求在状态机中被执行了两次，会违背线性一致性。</p><p>因此我们需要保证客户端的请求只能被状态机应用一次，我们可以维护一个去重哈希表，客户端 ID + 命令 ID 组成一个唯一的标识符，如果判断到命令是已经被执行过的，则直接返回对应的结果。</p><h3 id=shardkv-的问题为什么需要对分布式-kv-系统进行分片>Shardkv 的问题：为什么需要对分布式 KV 系统进行分片？</h3><p>一是单个 raft 集群实际存储数据的引擎是单机的，能够存储的数据量有限。二是在不分区的情况下，所有数据的读写请求都会在一个分片中，这在并发量较大的情况下可能存在一定的瓶颈。</p><p>如果对数据做了分区，那么不同分区之间的数据读写请求是可以并行的，这能够较大的提升 KV 系统的并发能力。</p><h3 id=shardkv-的配置怎么保存>Shardkv 的配置怎么保存？</h3><p>Shardkv 的配置是单独保存在一个服务中，客户端会向这个服务发起请求，查询 key 所属的 shard 应该在哪个 raft 集群中，并向这个集群发起请求。</p><p>配置服务也需要高可用特性，因为配置服务如果发生故障不可用的话，那么整个分布式 kv 服务都会无法提供服务，因此也使用 raft 算法保证高可用，构建了一个单 raft 集群来存储配置信息。</p><h3 id=shard-数据如何迁移>Shard 数据如何迁移？</h3><p>启动一个后台定时任务，定期从配置服务中获取最新的配置，如果检测到配置发生变更，则变更对应 shard 的状态，标记为需要进行迁移。</p><p>同时启动另一个后台定时任务，定期扫描 shard 的状态，如果检测到需要进行迁移的 shard，则发送消息，通过 raft 模块进行同步。然后在 Leader 节点中处理 shard 迁移的请求，将 shard 数据从原所属的 raft 集群中迁移到新的集群中。</p><h3 id=shard-迁移的时候客户端的请求会受到影响吗>Shard 迁移的时候，客户端的请求会受到影响吗？</h3><p>如果客户端请求的 key 所属的 shard 并没有在迁移中，那么可以正常提供服务。</p><p>否则，说明客户端请求的 key 在迁移中，则返回错误，让客户端进行重试。</p><h3 id=如果有并发的客户端请求和-shard-迁移请求应该怎么处理>如果有并发的客户端请求和 shard 迁移请求，应该怎么处理？</h3><p>客户端请求和 shard 迁移请求的确存在并发情况，如果处理顺序不一致，会违背线性一致性。</p><p>我们将 shard 迁移的请求也传入到 raft 模块进行同步，这样和客户端的请求是一致的，利用 raft 的一致性来保证两种不同请求的先后顺序，前面的执行结果一定对后续的请求可见。</p><h3 id=如果某个-shard-已经迁移了那么它还会占存储空间吗>如果某个 Shard 已经迁移了，那么它还会占存储空间吗？</h3><p>不会，我们实现了 shard 清理的完整流程，会启动一个后台定时任务，定期扫描 shard 的状态，如果检测到 shard 是需要进行清理的，则也会发送 shard 清理消息进行处理。</p><h2 id=参考资料>参考资料</h2><ol><li>Paxos vs Raft：https://ics.uci.edu/~cs237/reading/Paxos_vs_Raft.pdf</li><li>TiKV Raft 的优化：https://zhuanlan.zhihu.com/p/25735592</li><li>Raft FAQ：https://pdos.csail.mit.edu/6.824/papers/raft-faq.txt</li></ol></section><footer class=article-footer><section class=article-tags><a href=/tags/kv-%E5%AD%98%E5%82%A8/>KV 存储</a>
<a href=/tags/%E5%88%86%E5%B8%83%E5%BC%8F-kv/>分布式 KV</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F-kv%E8%AF%BE%E7%A8%8B%E5%AE%8C%E7%BB%93/><div class=article-details><h2 class=article-title>从零实现分布式 KV——课程完结！</h2></div></a></article><article><a href=/p/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F-kv%E8%AF%BE%E7%A8%8B%E6%9B%B4%E6%96%B0/><div class=article-details><h2 class=article-title>从零实现分布式 KV——课程更新</h2></div></a></article><article><a href=/p/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-kv-%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F-kv-%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/><div class=article-details><h2 class=article-title>从零实现 KV 和分布式 KV 有什么区别？</h2></div></a></article><article><a href=/p/%E9%87%8D%E7%A3%85%E5%87%BA%E7%82%89%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F-kv/><div class=article-details><h2 class=article-title>重磅出炉！从零实现分布式 KV！</h2></div></a></article><article><a href=/p/%E7%AE%80%E5%8E%86%E4%B8%8A%E5%86%99%E4%BB%80%E4%B9%88%E9%A1%B9%E7%9B%AE%E6%A0%A1%E6%8B%9B%E8%83%BD%E6%8B%BF%E5%88%B0-30w-%E7%9A%84-offer/><div class=article-details><h2 class=article-title>简历上写什么项目，校招能拿到 30w 的 offer</h2></div></a></article></div></div></aside><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js></script><script src=https://cdn.jsdelivr.net/npm/blueimp-md5@2.18.0/js/md5.min.js></script><script>const gitalk=new Gitalk({clientID:"354d1c483644032e3d21",clientSecret:"3f65d24831ac79f4e68f762f5f02e4a75ae40ad3",repo:"roseduan.github.io",owner:"roseduan",admin:["roseduan"],distractionFreeMode:!1,id:md5(location.pathname),proxy:null});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("gitalk-container").innerHTML="Gitalk comments not available by default when the website is previewed locally.";return}gitalk.render("gitalk-container")})()</script><footer class=site-footer><section class=copyright>&copy;
2024 roseduan</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>